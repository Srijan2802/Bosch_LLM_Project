{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf0DkdTVB7xR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Load category links\n",
        "df = pd.read_excel(\"Category_links.xlsx\")\n",
        "\n",
        "# Define headers\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Lists to store data\n",
        "data = []\n",
        "\n",
        "\n",
        "def get_pagination_links(category_url):\n",
        "    \"\"\"Extracts all pagination links from a category page.\"\"\"\n",
        "    page_urls = set([category_url])\n",
        "    current_url = category_url\n",
        "\n",
        "    while True:\n",
        "        response = requests.get(current_url, headers=HEADERS, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Find all pagination links\n",
        "        pagination_links = soup.select(\".m-ghostblock__nav-item[href]\")\n",
        "        next_page_button = soup.select_one(\".m-ghostblock__nav-item.arrow[data-href]\")\n",
        "\n",
        "        # Add pagination links\n",
        "        for link in pagination_links:\n",
        "            url = link[\"href\"].strip()\n",
        "            if url and url not in page_urls:\n",
        "                page_urls.add(url)\n",
        "\n",
        "        # Handle \"Next\" button\n",
        "        if next_page_button:\n",
        "            next_url = next_page_button[\"data-href\"].strip()\n",
        "            if next_url and next_url not in page_urls:\n",
        "                page_urls.add(next_url)\n",
        "                current_url = next_url\n",
        "                time.sleep(1)\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return list(page_urls)\n",
        "\n",
        "\n",
        "def scrape_category(category_title, category_url):\n",
        "    \"\"\"Scrapes all products from a category and its paginated pages.\"\"\"\n",
        "    for page_url in get_pagination_links(category_url):\n",
        "        print(f\"Scraping category page: {page_url}\")\n",
        "        response = requests.get(page_url, headers=HEADERS, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        for product in soup.select(\".category-grid-tile.a-category-grid-tile\"):\n",
        "            link_tag = product.select_one(\".category-grid-tile__link-wrapper.trackingElement\")\n",
        "            if link_tag:\n",
        "                product_name = link_tag.get(\"title\", \"\").strip()\n",
        "                product_url = link_tag.get(\"href\", \"\").strip()\n",
        "\n",
        "                if product_url and not product_url.startswith(\"http\"):\n",
        "                    product_url = \"https://www.boschtools.com\" + product_url\n",
        "\n",
        "                # Scrape product details\n",
        "                product_details = scrape_product_details(product_url)\n",
        "\n",
        "                # Save data\n",
        "                data.append({\n",
        "                    \"Category Name\": category_title,\n",
        "                    \"Category Link\": category_url,\n",
        "                    \"Product Name\": product_name,\n",
        "                    \"Product Link\": product_url,\n",
        "                    **product_details\n",
        "                })\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "def scrape_product_details(product_url):\n",
        "    \"\"\"Extracts all available technical data metrics from the product page.\"\"\"\n",
        "    response = requests.get(product_url, headers=HEADERS, timeout=10)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    tech_data = {}\n",
        "\n",
        "    # Locate the technical data table\n",
        "    table = soup.select_one(\".o-technical_data table\")\n",
        "\n",
        "    if table:\n",
        "        for row in table.select(\"tbody tr\"):\n",
        "            cells = row.find_all(\"td\")\n",
        "            if len(cells) == 2:\n",
        "                metric = cells[0].text.strip()\n",
        "                value = cells[1].text.strip()\n",
        "                tech_data[metric] = value\n",
        "\n",
        "    return tech_data\n",
        "\n",
        "\n",
        "# Scrape all categories\n",
        "for _, row in df.iterrows():\n",
        "    scrape_category(row[\"Title\"], row[\"Links\"])\n",
        "    time.sleep(2)\n",
        "\n",
        "# Save data to Excel\n",
        "pd.DataFrame(data).to_excel(\"Bosch_Products_All_Metrics.xlsx\", index=False)\n",
        "print(\"Scraping complete. Data saved to Bosch_Products_All_Metrics.xlsx\")\n"
      ]
    }
  ]
}